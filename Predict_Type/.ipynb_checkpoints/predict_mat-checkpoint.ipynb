{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df.drop([\"UsageClass\", \"CheckoutType\", \"CheckoutYear\", \"CheckoutMonth\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Checkouts</th>\n",
       "      <th>Title</th>\n",
       "      <th>Creator</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>PublicationYear</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Tidal wave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tsunamis, Tsunamis Juvenile literature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>London holiday / Richard Peck.</td>\n",
       "      <td>Peck, Richard, 1934-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Viking,</td>\n",
       "      <td>1998.</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Cinco de Mayo : celebrating Hispanic pride / C...</td>\n",
       "      <td>Gnojewski, Carol</td>\n",
       "      <td>Cinco de Mayo Mexican holiday History Juvenile...</td>\n",
       "      <td>Enslow Publishers,</td>\n",
       "      <td>c2002.</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>War stories, Historical fiction, Domestic fict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>As a man thinketh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thought and thinking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Checkouts                                              Title  \\\n",
       "0   1          1                                         Tidal wave   \n",
       "1   2          1                     London holiday / Richard Peck.   \n",
       "2   3          3  Cinco de Mayo : celebrating Hispanic pride / C...   \n",
       "3   4          1                                          Annapolis   \n",
       "4   5          1                                  As a man thinketh   \n",
       "\n",
       "                Creator                                           Subjects  \\\n",
       "0                   NaN             Tsunamis, Tsunamis Juvenile literature   \n",
       "1  Peck, Richard, 1934-                                                NaN   \n",
       "2      Gnojewski, Carol  Cinco de Mayo Mexican holiday History Juvenile...   \n",
       "3                   NaN  War stories, Historical fiction, Domestic fict...   \n",
       "4                   NaN                               Thought and thinking   \n",
       "\n",
       "            Publisher PublicationYear MaterialType  \n",
       "0                 NaN             NaN         BOOK  \n",
       "1             Viking,           1998.         BOOK  \n",
       "2  Enslow Publishers,          c2002.         BOOK  \n",
       "3                 NaN             NaN         BOOK  \n",
       "4                 NaN             NaN         BOOK  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23137\n",
      "1763\n",
      "21916\n",
      "21931\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Creator\"].isna().sum())\n",
    "print(df[\"Subjects\"].isna().sum())\n",
    "print(df[\"Publisher\"].isna().sum())\n",
    "print(df[\"PublicationYear\"].isna().sum())\n",
    "print(df[\"Title\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NaN values are more than 70%\n",
    "df = df.drop([\"Creator\", \"Publisher\", \"PublicationYear\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Checkouts</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Tidal wave</td>\n",
       "      <td>Tsunamis, Tsunamis Juvenile literature</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>London holiday / Richard Peck.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Cinco de Mayo : celebrating Hispanic pride / C...</td>\n",
       "      <td>Cinco de Mayo Mexican holiday History Juvenile...</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>War stories, Historical fiction, Domestic fict...</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>As a man thinketh</td>\n",
       "      <td>Thought and thinking</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Checkouts                                              Title  \\\n",
       "0   1          1                                         Tidal wave   \n",
       "1   2          1                     London holiday / Richard Peck.   \n",
       "2   3          3  Cinco de Mayo : celebrating Hispanic pride / C...   \n",
       "3   4          1                                          Annapolis   \n",
       "4   5          1                                  As a man thinketh   \n",
       "\n",
       "                                            Subjects MaterialType  \n",
       "0             Tsunamis, Tsunamis Juvenile literature         BOOK  \n",
       "1                                                NaN         BOOK  \n",
       "2  Cinco de Mayo Mexican holiday History Juvenile...         BOOK  \n",
       "3  War stories, Historical fiction, Domestic fict...         BOOK  \n",
       "4                               Thought and thinking         BOOK  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nan_id =[]\n",
    "for i in range(len(df)):\n",
    "    if str(df[\"Subjects\"][i]) == \"nan\":\n",
    "        nan_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nlp = df[\"Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text processing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "\n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    ## Remove stop words\n",
    "    text = [w for w in text if not w in sw and len(w) >= 3]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    txt = text.split()\n",
    "    text = [i for i in txt if i.isalpha()]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nlp = df_nlp.apply(clean_text)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=1000,\n",
    "strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "tfv.fit(list(df_nlp))\n",
    "desc = tfv.transform(df_nlp)\n",
    "\n",
    "# truncate the matrix/ array to 30 column\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "svd.fit(desc)\n",
    "desc = svd.transform(desc)\n",
    "\n",
    "desc = pd.DataFrame(desc, columns=['nlp_{}'.format(i) for i in range(50)])\n",
    "\n",
    "final_df = pd.concat([df, desc],axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = final_df.iloc[:, 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "final_df[\"MaterialType\"] = le.fit_transform(final_df[\"MaterialType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = final_df[\"MaterialType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=2, random_state=0)\n",
    "\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "Y_pred = Y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_3 = accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7148091988880465"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_file.csv\")\n",
    "\n",
    "test_data=test_data.drop([\"UsageClass\", \"CheckoutType\", \"CheckoutYear\", \n",
    "                          \"CheckoutMonth\",\"Creator\", \"Publisher\", \"PublicationYear\"], axis=1)\n",
    "\n",
    "test_data[\"Title\"] = test_data[\"Title\"].apply(clean_text)\n",
    "\n",
    "desc_test = tfv.transform(test_data[\"Title\"])\n",
    "desc_test = svd.transform(desc_test)\n",
    "desc_test = pd.DataFrame(desc_test, columns=['nlp_{}'.format(i) for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_Y_pred = clf.predict(desc_test)\n",
    "\n",
    "test_Y_pred = test_Y_pred.tolist()\n",
    "final_Y = le.inverse_transform(test_Y_pred)\n",
    "final = final_Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({\"MaterialType\":final})\n",
    "submit.insert(loc=0, column='ID', value=test_data[\"ID\"])\n",
    "#submit.insert(loc=1, column='MaterialType', value=test_data[\"Essayset\"])\n",
    "\n",
    "#submit.to_csv('submission_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submission_LR.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0).fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "acc_DT = accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65390447308567101"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_3 = accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74323982815264089"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "lr = [0.01,0.1,0.4]\n",
    "estimator = [100, 300, 500, 700, 100]\n",
    "for i in lr:\n",
    "    for j in estimator:\n",
    "        xgb = XGBClassifier(learning_rate=i,n_estimators=j, max_depth=3,booster='dart')\n",
    "        xgb.fit(X_train, Y_train)\n",
    "        Y_pred_xgb = xgb.predict(X_test)\n",
    "        Y_pred_xgb = Y_pred_xgb.tolist()\n",
    "        acc_1 = accuracy_score(Y_test, Y_pred_xgb)\n",
    "        print (\"lr and estimator\", i, j, acc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from fine tuning we get to know, learning rate = 0.1 and n_estimator =300 gives max accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1,n_estimators=300, max_depth=3,booster='dart')\n",
    "xgb.fit(X_train, Y_train)\n",
    "Y_pred_xgb = xgb.predict(X_test)\n",
    "Y_pred_xgb = Y_pred_xgb.tolist()\n",
    "acc_1 = accuracy_score(Y_test, Y_pred_xgb)\n",
    "\n",
    "#Y_pred_xgb = model.predict(X_test)\n",
    "Y_pred_xgb = xgb.predict(X_test)\n",
    "Y_pred_xgb = Y_pred_xgb.tolist()\n",
    "\n",
    "acc_1 = accuracy_score(Y_test, Y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75991913065453631"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75650745514278495"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_Y_xgb = model.predict(desc_test)\n",
    "test_Y_xgb = xgb.predict(desc_test)\n",
    "#test_Y_cat =  test_Y_xgb.tolist()\n",
    "#test_Y_xgb = [int(test_Y_cat[i][0]) for i in range(len(test_Y_cat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "final_Y = le.inverse_transform(test_Y_xgb)\n",
    "final = final_Y.tolist()\n",
    "submit = pd.DataFrame({\"MaterialType\":final})\n",
    "submit.insert(loc=0, column='ID', value=test_data[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submission_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           Tidal wave\n",
       "1                       London holiday / Richard Peck.\n",
       "2    Cinco de Mayo : celebrating Hispanic pride / C...\n",
       "3                                            Annapolis\n",
       "4                                    As a man thinketh\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2 = df[\"Title\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2 = df_2.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25186 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df_2.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (31653, 150)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df_2.values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y =df[\"MaterialType\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (31653, 8)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['MaterialType']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28487, 150) (28487, 8)\n",
      "(3166, 150) (3166, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train_L, X_test_L, Y_train_L, Y_test_L = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train_L.shape,Y_train_L.shape)\n",
    "print(X_test_L.shape,Y_test_L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25638 samples, validate on 2849 samples\n",
      "Epoch 1/5\n",
      "25638/25638 [==============================] - 229s 9ms/step - loss: 0.9594 - acc: 0.7143 - val_loss: 0.7777 - val_acc: 0.7564\n",
      "Epoch 2/5\n",
      "25638/25638 [==============================] - 221s 9ms/step - loss: 0.6826 - acc: 0.7758 - val_loss: 0.7533 - val_acc: 0.7687\n",
      "Epoch 3/5\n",
      "25638/25638 [==============================] - 209s 8ms/step - loss: 0.5341 - acc: 0.8220 - val_loss: 0.7986 - val_acc: 0.7518\n",
      "Epoch 4/5\n",
      "25638/25638 [==============================] - 221s 9ms/step - loss: 0.4200 - acc: 0.8568 - val_loss: 0.8787 - val_acc: 0.7301\n",
      "Epoch 5/5\n",
      "25638/25638 [==============================] - 237s 9ms/step - loss: 0.3340 - acc: 0.8851 - val_loss: 0.9959 - val_acc: 0.7217\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_L, Y_train_L, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOK', 'SOUNDDISC', 'VIDEOCASS', 'VIDEODISC', 'SOUNDCASS', 'MUSIC',\n",
       "       'MIXED', 'CR'], dtype=object)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"MaterialType\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MUSIC'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(Y_train_L[7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3166/3166 [==============================] - 2s 501us/step\n",
      "Test set\n",
      "  Loss: 0.988\n",
      "  Accuracy: 0.724\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test_L,Y_test_L)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data[\"Title\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst = tokenizer.texts_to_sequences(test_data.values)\n",
    "tst = pad_sequences(tst, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21102"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = list(df[\"MaterialType\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MUSIC'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(pred[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = []\n",
    "for i in range(len(pred)):\n",
    "    final.append(labels[np.argmax(pred[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({\"MaterialType\":final})\n",
    "submit.insert(loc=0, column='ID', value=test_data[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submission_L2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is fitted: True\n",
      "Model params:\n",
      "{'depth': 2, 'iterations': 200, 'loss_function': 'MultiClass', 'learning_rate': 1}\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=1,\n",
    "    loss_function='MultiClass',\n",
    "    depth = 2\n",
    "    # loss_function='CrossEntropy'\n",
    ")\n",
    "model.fit(\n",
    "    X_train, Y_train,\n",
    "    #cat_features=cat_features,\n",
    "    eval_set=(X_test, Y_test),\n",
    "    verbose=False\n",
    "    \n",
    ")\n",
    "print('Model is fitted: ' + str(model.is_fitted()))\n",
    "print('Model params:')\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = Y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_3 = accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71973717462724285"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
